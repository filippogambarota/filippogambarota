{
  "hash": "28bcab6970d5ee3fd3e062ef60206448",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"BRMS Notes\"\ndate: \"2024-01-28\"\ncategories: [bayesian]\n---\n\n\n\n\n# The model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- brm(mpg ~ disp + vs, data = mtcars)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.9e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.19 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.069 seconds (Warm-up)\nChain 1:                0.015 seconds (Sampling)\nChain 1:                0.084 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 2e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.058 seconds (Warm-up)\nChain 2:                0.015 seconds (Sampling)\nChain 2:                0.073 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.066 seconds (Warm-up)\nChain 3:                0.017 seconds (Sampling)\nChain 3:                0.083 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.05 seconds (Warm-up)\nChain 4:                0.017 seconds (Sampling)\nChain 4:                0.067 seconds (Total)\nChain 4: \n```\n\n\n:::\n:::\n\n\n# Predict Function\n\nE' importante mettere il `seed` perchè anche senza `newdata` i risultati sono leggermente diversi.\n\n* la funzione `predict()` di default mette `re_formula = NULL` ovvero considera anche i **group-level** effects (effetti random). In questo modello non ci sono effetti random quindi `re_formula = NULL` e `re_formula = NA` (cosi si sopprimono gli effetti random) sono la stessa cosa. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(22)\n\npredict(fit, re_formula = NULL) %>% # Random effect considered\n  tibble()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 32 × 1\n   .[,\"Estimate\"] [,\"Est.Error\"] [,\"Q2.5\"] [,\"Q97.5\"]\n            <dbl>          <dbl>     <dbl>      <dbl>\n 1           22.0           3.64     14.8        29.0\n 2           21.9           3.69     14.4        29.1\n 3           25.4           3.52     18.6        32.5\n 4           20.0           3.60     12.8        27.1\n 5           14.7           3.55      7.61       21.8\n 6           21.1           3.63     13.9        28.3\n 7           14.7           3.50      7.84       21.5\n 8           24.0           3.54     17.1        30.9\n 9           24.3           3.50     17.5        31.1\n10           23.3           3.53     16.3        30.2\n# ℹ 22 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\nset.seed(22)\n\npredict(fit, re_formula = NA) %>% # Random effect excluded\n  tibble()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 32 × 1\n   .[,\"Estimate\"] [,\"Est.Error\"] [,\"Q2.5\"] [,\"Q97.5\"]\n            <dbl>          <dbl>     <dbl>      <dbl>\n 1           22.0           3.64     14.8        29.0\n 2           21.9           3.69     14.4        29.1\n 3           25.4           3.52     18.6        32.5\n 4           20.0           3.60     12.8        27.1\n 5           14.7           3.55      7.61       21.8\n 6           21.1           3.63     13.9        28.3\n 7           14.7           3.50      7.84       21.5\n 8           24.0           3.54     17.1        30.9\n 9           24.3           3.50     17.5        31.1\n10           23.3           3.53     16.3        30.2\n# ℹ 22 more rows\n```\n\n\n:::\n:::\n\n\n# Fitted\n\nLa funzione `fitted()` non prende in considerazione la **varianza residua** e quindi semplicemente riporta il **sampling dalla posterior** (se l'argomento `newdata` è missing). Quindi riporta il valore fittato dal modello per ogni osservazione.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(22)\n\nfitted(fit) %>% \n  tibble()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 32 × 1\n   .[,\"Estimate\"] [,\"Est.Error\"] [,\"Q2.5\"] [,\"Q97.5\"]\n            <dbl>          <dbl>     <dbl>      <dbl>\n 1           22.0          1.29       19.4       24.5\n 2           22.0          1.29       19.4       24.5\n 3           25.4          0.936      23.6       27.3\n 4           19.9          1.28       17.4       22.5\n 5           14.7          0.883      12.9       16.4\n 6           21.1          1.13       18.9       23.4\n 7           14.7          0.883      12.9       16.4\n 8           24.0          0.930      22.2       25.8\n 9           24.2          0.926      22.4       26.0\n10           23.2          0.959      21.4       25.1\n# ℹ 22 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\npredict(fit) %>% # wider CI given the residual variance\n  tibble()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 32 × 1\n   .[,\"Estimate\"] [,\"Est.Error\"] [,\"Q2.5\"] [,\"Q97.5\"]\n            <dbl>          <dbl>     <dbl>      <dbl>\n 1           22.0           3.64     14.8        29.0\n 2           21.9           3.69     14.4        29.1\n 3           25.4           3.52     18.6        32.5\n 4           20.0           3.60     12.8        27.1\n 5           14.7           3.55      7.61       21.8\n 6           21.1           3.63     13.9        28.3\n 7           14.7           3.50      7.84       21.5\n 8           24.0           3.54     17.1        30.9\n 9           24.3           3.50     17.5        31.1\n10           23.3           3.53     16.3        30.2\n# ℹ 22 more rows\n```\n\n\n:::\n:::\n\n\nLa funzione `posterior_epred()` o `posterior_linepred()` hanno due proprietà distintive:\n\n* non tengono in considerazione la **varianza residua** (come `fitted()`)\n* trasformano i valori predetti usando la **link function**\n\nPer far equivalere `fitted()` e `posterior_linepred()` o `posterior_epred()` bisogna aggiungere `summary = FALSE` a `fitted()` in modo restituisca tutte le iterazioni.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(22)\n\nfitted(fit, summary = FALSE)[1:10]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 20.48674 20.60826 20.21701 20.60638 22.05136 21.81278 23.69372 20.27163\n [9] 21.93875 21.56554\n```\n\n\n:::\n\n```{.r .cell-code}\nposterior_epred(fit)[1:10]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 20.48674 20.60826 20.21701 20.60638 22.05136 21.81278 23.69372 20.27163\n [9] 21.93875 21.56554\n```\n\n\n:::\n\n```{.r .cell-code}\nposterior_linpred(fit)[1:10]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 20.48674 20.60826 20.21701 20.60638 22.05136 21.81278 23.69372 20.27163\n [9] 21.93875 21.56554\n```\n\n\n:::\n:::\n\n\nCi sono altri argomenti della funzione ma l'aspetto principale è considerare o meno la varianza residua. Summary:\n\n* `fitted()`: estrae dalla posterior i valori considerando solo l'**incertezza della media** con o senza effetti random (group-level effect; `re_formula = NA`)\n  * `posterior_epred()` e `posterior_linpred()` fanno la stessa cosa ma restituiscono tutte le iterazioni e trasformano la variabile con la *link function*\n* `predict()`: estrae dalla **posterior predictive distribution** tenendo in considerazione tutte le fonti di variabilità. Anche predict può considerare o meno i **group-level effect** usando `re_formula =`\n  * `posterior_predict()` fa la stessa cosa ma restituisce tutte le iterazioni\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}